{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0787b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import gridspec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6abc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "data = pd.read_csv(\"2.creditcard.csv\")\n",
    "# Understanding the code\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ac8b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the data\n",
    "# shape of the data\n",
    "print(data.shape) \n",
    "print(data.describe()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6365dd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imbalance in the data\n",
    "# Determine number of fraud cases in dataset\n",
    "fraud = data[data['Class'] == 1]\n",
    "valid = data[data['Class'] == 0]\n",
    "outlierFraction = len(fraud)/float(len(valid))\n",
    "print(outlierFraction)\n",
    "print('Fraud Cases: {}'.format(len(data[data['Class'] == 1])))\n",
    "print('Valid Transactions: {}'.format(len(data[data['Class'] == 0])))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b0e93c3",
   "metadata": {},
   "source": [
    "# Only 0.17% fraudulent transaction out all the transactions. The data is highly Unbalanced. Lets first apply our models without balancing it and if we don’t get a good accuracy then we can find a way to balance this dataset. But first, let’s implement the model without it and will balance the data only if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3183a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Print the amount details for Fraudulent Transaction\n",
    "print(\"Amount details of the fraudulent transaction\")\n",
    "fraud.Amount.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad881a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the amount details for Normal Transaction\n",
    "print(\"details of valid transactions\")\n",
    "valid.Amount.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d313c1b6",
   "metadata": {},
   "source": [
    "As we can clearly notice from this, the average Money transaction for the fraudulent ones is more. This makes this problem crucial to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d8fc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the correlation Matrix\n",
    "corrmat = data.corr()\n",
    "fig = plt.figure(figsize = (12,9))\n",
    "sns.heatmap(corrmat, vmax = .8, square = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "431b674a",
   "metadata": {},
   "source": [
    "In the HeatMap we can clearly see that most of the features do not correlate to other features but there are some features that either has a positive or a negative correlation with each other. For example, V2 and V5 are highly negatively correlated with the feature called Amount. We also see some correlation with V20 and Amount. This gives us a deeper understanding of the Data available to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f336bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the X and the Y values\n",
    "X = data.drop(['Class'], axis = 1)\n",
    "Y = data[\"Class\"]\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "# getting just the values for the sake of processing  \n",
    "# (its a numpy array with no columns) \n",
    "xData = X.values \n",
    "yData = Y.values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b77a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Scikit-Learn to split data unto traning and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# split the data into training and testing sets\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(xData, yData, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcc684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Random Forest Classifier (RANDOM FOREST)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# random forest model creation\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(xTrain, yTrain)\n",
    "\n",
    "# Predictions\n",
    "yPred = rfc.predict(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41683c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building all kinds of evaluating parameters\n",
    "# Evaluating the classifier \n",
    "# printing every scoreof the classifier \n",
    "# scoring in anything \n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "n_outliers = len(fraud)\n",
    "n_errors = (yPred != yTest).sum()\n",
    "print(\"The model used is Random Forest Classifier\")\n",
    "\n",
    "acc = accuracy_score(yTest, yPred)\n",
    "print(\"The accuracy is {}\".format(acc))\n",
    "\n",
    "prec = precision_score(yTest, yPred) \n",
    "print(\"The precision is {}\".format(prec)) \n",
    "  \n",
    "rec = recall_score(yTest, yPred) \n",
    "print(\"The recall is {}\".format(rec)) \n",
    "  \n",
    "f1 = f1_score(yTest, yPred) \n",
    "print(\"The F1-Score is {}\".format(f1)) \n",
    "  \n",
    "MCC = matthews_corrcoef(yTest, yPred) \n",
    "print(\"The Matthews correlation coefficient is{}\".format(MCC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4f166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the confusion matrix \n",
    "LABELS = ['Normal', 'Fraud']\n",
    "conf_matrix = confusion_matrix(yTest, yPred)\n",
    "plt.figure(figsize = (12, 12))\n",
    "sns.heatmap(conf_matrix, xticklabels = LABELS,\n",
    "            yticklabels = LABELS, annot = True, fmt=\"d\");\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel('True class')\n",
    "plt.xlabel('Predicted class')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
